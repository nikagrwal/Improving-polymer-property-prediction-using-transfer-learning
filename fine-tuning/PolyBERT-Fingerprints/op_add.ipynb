{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from psmiles import PolymerSmiles as PS\n",
    "from sklearn.metrics  import  mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from optuna.trial import TrialState \n",
    "\n",
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/updated_polymers.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2019405/2183922834.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  data_train_tensor = torch.tensor(data_train.reset_index(drop = True), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "scalar = MinMaxScaler()\n",
    "data = df[\"fingerprint_polyBERT\"]\n",
    "target = df[\"Egc\"]\n",
    "\n",
    "#data = data.values.reshape(-1, 1)  # Reshape data\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=123)\n",
    "\n",
    "# Scaling target variable\n",
    "target_train = scalar.fit_transform(target_train.values.reshape(-1, 1))\n",
    "target_test = scalar.transform(target_test.values.reshape(-1, 1))\n",
    "\n",
    "# Creating tensors from data\n",
    "\n",
    "#Training Data\n",
    "data_train_tensor = torch.tensor(data_train.reset_index(drop = True), dtype=torch.float32)\n",
    "target_train_tensor = torch.tensor(target_train, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(data_train_tensor, target_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size= 32, shuffle= True)\n",
    "\n",
    "#Testing Data\n",
    "\n",
    "data_test_tensor = torch.tensor(data_test.reset_index(drop= True), dtype=torch.float32)\n",
    "target_test_tensor = torch.tensor(target_test, dtype=torch.float32)\n",
    "\n",
    "test_dataset = TensorDataset(data_test_tensor, target_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net():\n",
    "    layers =  nn.ModuleList([\n",
    "           nn.Sequential(\n",
    "                nn.Linear(600, 1504),\n",
    "                nn.Dropout(0.122517721),\n",
    "                nn.PReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1504, 1760),\n",
    "                nn.Dropout(0.125659318),\n",
    "                nn.PReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1760, 736),\n",
    "                nn.Dropout(0.125674157),\n",
    "                nn.PReLU()\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('my_layers.0.0.weight', tensor([[-0.0063, -0.0108,  0.1236,  ...,  0.0241,  0.1130,  0.0365],\n",
      "        [ 0.0888, -0.0701,  0.1512,  ..., -0.0178,  0.0828, -0.0632],\n",
      "        [ 0.0075,  0.0749, -0.1049,  ...,  0.0371,  0.0677, -0.0243],\n",
      "        ...,\n",
      "        [ 0.0657,  0.0548,  0.0581,  ...,  0.0889,  0.0037,  0.0107],\n",
      "        [-0.1599, -0.0155, -0.0027,  ...,  0.0125, -0.0211, -0.1373],\n",
      "        [-0.2703, -0.0034, -0.0578,  ...,  0.0481,  0.0143, -0.0188]],\n",
      "       device='cuda:3')), ('my_layers.0.0.bias', tensor([-0.0520, -0.0220, -0.0389,  ..., -0.0527, -0.0120, -0.0783],\n",
      "       device='cuda:3')), ('my_layers.0.2.weight', tensor([0.0144], device='cuda:3')), ('my_layers.1.0.weight', tensor([[ 0.0419, -0.0316,  0.1388,  ..., -0.0146, -0.1075, -0.0230],\n",
      "        [-0.0773, -0.0028,  0.0360,  ...,  0.0285, -0.1143, -0.0702],\n",
      "        [-0.2174,  0.0222, -0.0673,  ..., -0.0390, -0.0597, -0.0100],\n",
      "        ...,\n",
      "        [-0.2156, -0.0982, -0.0056,  ..., -0.0293, -0.0807, -0.0047],\n",
      "        [ 0.0783, -0.0809,  0.0748,  ..., -0.1426,  0.0530, -0.0794],\n",
      "        [-0.0841, -0.0834,  0.0204,  ..., -0.0095,  0.1051, -0.1078]],\n",
      "       device='cuda:3')), ('my_layers.1.0.bias', tensor([-0.2006, -0.2248, -0.0516,  ...,  0.0189, -0.1925, -0.1454],\n",
      "       device='cuda:3')), ('my_layers.1.2.weight', tensor([0.0027], device='cuda:3')), ('my_layers.2.0.weight', tensor([[-0.0518,  0.0601,  0.1136,  ...,  0.0180,  0.0587,  0.0361],\n",
      "        [ 0.0168,  0.0392,  0.1020,  ..., -0.0303,  0.0212,  0.0498],\n",
      "        [-0.0020,  0.0257,  0.0601,  ...,  0.0200, -0.0035,  0.0168],\n",
      "        ...,\n",
      "        [-0.0404,  0.0246,  0.0899,  ...,  0.0266,  0.0041,  0.0712],\n",
      "        [ 0.0300, -0.0580,  0.0336,  ..., -0.0376, -0.0311,  0.0084],\n",
      "        [ 0.0353,  0.0186, -0.0040,  ..., -0.0051, -0.0209, -0.1024]],\n",
      "       device='cuda:3')), ('my_layers.2.0.bias', tensor([-0.4085, -0.3929, -0.1542, -0.4921,  0.3618,  0.3140, -0.0923, -0.0275,\n",
      "         0.2511, -0.4732, -0.2615, -0.4232,  0.3670, -0.3575,  0.2817,  0.3870,\n",
      "         0.2940, -0.3923, -0.3511, -0.3326,  0.1643, -0.3841,  0.2682, -0.0032,\n",
      "         0.2661,  0.3121,  0.3086, -0.4111,  0.2954,  0.3307, -0.4169,  0.3284,\n",
      "        -0.4011, -0.3687, -0.1441, -0.3689,  0.3288, -0.4769,  0.0849, -0.4112,\n",
      "        -0.4077,  0.0277, -0.4010, -0.4620, -0.4330, -0.3734,  0.3735, -0.2402,\n",
      "        -0.2308, -0.2054, -0.4250,  0.3069,  0.2981, -0.3794,  0.3090,  0.2341,\n",
      "        -0.3598, -0.2008, -0.3879, -0.4127,  0.3365,  0.3874, -0.2174,  0.2961,\n",
      "        -0.2229, -0.3339, -0.1652, -0.3973,  0.3131,  0.3382,  0.0939, -0.3344,\n",
      "        -0.3357,  0.2704, -0.3997, -0.0177,  0.4007, -0.0308,  0.3544, -0.5006,\n",
      "        -0.4251,  0.3150, -0.1929, -0.2804, -0.3808,  0.2884, -0.4036, -0.3419,\n",
      "         0.3476, -0.4360,  0.2792,  0.1121,  0.2725, -0.5291, -0.3607, -0.1027,\n",
      "        -0.1632, -0.5094,  0.3055,  0.2934, -0.2121, -0.4259, -0.3990,  0.2400,\n",
      "        -0.0875,  0.2764, -0.4201, -0.2293, -0.2760, -0.4133,  0.3698, -0.3287,\n",
      "        -0.2871, -0.2435,  0.0322, -0.1273,  0.3328,  0.3381, -0.3683,  0.2221,\n",
      "         0.1355, -0.4071, -0.4478, -0.3984, -0.3646, -0.3930,  0.2606,  0.2659,\n",
      "         0.3591, -0.3694, -0.3869,  0.3354, -0.3396, -0.2325,  0.3113, -0.2439,\n",
      "         0.4664, -0.3873, -0.4214, -0.4560, -0.4161,  0.3264,  0.2702, -0.2500,\n",
      "         0.1755, -0.4141,  0.3096,  0.3688,  0.2055, -0.0378, -0.0293, -0.3570,\n",
      "        -0.4328, -0.4383, -0.3802, -0.4056, -0.4142,  0.2549, -0.4148,  0.0173,\n",
      "        -0.3902,  0.0538, -0.3989, -0.0521,  0.3399, -0.2184,  0.3455, -0.3472,\n",
      "        -0.4101,  0.2705,  0.3684, -0.0517,  0.3686,  0.3596,  0.2437,  0.1454,\n",
      "        -0.3959,  0.3296,  0.2332,  0.3640,  0.1538,  0.3647, -0.3577,  0.3817,\n",
      "        -0.5370, -0.4007,  0.1175, -0.4104, -0.4850, -0.4304, -0.2227, -0.1715,\n",
      "         0.3614, -0.3762, -0.2080, -0.3437,  0.0029, -0.3864, -0.0037, -0.4336,\n",
      "        -0.3417,  0.2876, -0.4147, -0.4068, -0.4114,  0.0352, -0.1102,  0.3296,\n",
      "         0.0636, -0.0803,  0.2254, -0.1172, -0.3826, -0.3843, -0.2458, -0.3506,\n",
      "        -0.4085, -0.4065,  0.3520, -0.3676, -0.4098, -0.3889, -0.3066, -0.3649,\n",
      "        -0.3140,  0.1693, -0.3463, -0.3815, -0.4173, -0.5185, -0.2193, -0.3674,\n",
      "         0.3847,  0.2780,  0.2428,  0.0583,  0.3272, -0.3950, -0.3726, -0.3877,\n",
      "         0.2362, -0.3803, -0.2940, -0.0357,  0.2950, -0.4014, -0.4129, -0.2573,\n",
      "        -0.2386,  0.3552,  0.3525,  0.1569,  0.2373,  0.3297,  0.2784, -0.2577,\n",
      "         0.2211,  0.3655,  0.3298,  0.3694, -0.3621, -0.3267, -0.6421, -0.3081,\n",
      "        -0.4121,  0.3120, -0.3515,  0.3625, -0.3998,  0.3581, -0.4140, -0.4027,\n",
      "        -0.3594,  0.3892, -0.3727,  0.3261, -0.3960,  0.2744,  0.2973, -0.4254,\n",
      "        -0.3811, -0.4137, -0.1568, -0.2353, -0.5540, -0.4442,  0.0196,  0.2461,\n",
      "         0.0342, -0.4145, -0.3907, -0.4005,  0.3598,  0.2523,  0.3010, -0.3222,\n",
      "         0.2643, -0.4321,  0.2883, -0.0138, -0.2523,  0.3415, -0.6162, -0.4057,\n",
      "        -0.0149, -0.3736, -0.0774, -0.4893, -0.3774, -0.3856,  0.2521, -0.4231,\n",
      "        -0.4270, -0.3989, -0.5210, -0.4161, -0.4041, -0.4274, -0.4262,  0.3346,\n",
      "         0.2697, -0.4154, -0.3238,  0.5910,  0.2757, -0.3454, -0.3810,  0.1991,\n",
      "         0.3056, -0.3755, -0.3766,  0.3832,  0.2540,  0.0198,  0.0892, -0.2320,\n",
      "        -0.3991, -0.4308,  0.2854,  0.2049, -0.3871, -0.4172, -0.0051, -0.2241,\n",
      "         0.2640,  0.2533,  0.2854, -0.3489,  0.3355, -0.2541, -0.1688,  0.3533,\n",
      "         0.0371,  0.2857,  0.3364, -0.0086, -0.3885,  0.2800, -0.2892, -0.4121,\n",
      "        -0.3856,  0.3681, -0.4190,  0.0927,  0.3082,  0.0444,  0.3225, -0.5436,\n",
      "        -0.4684, -0.3456, -0.3631,  0.2731, -0.4270,  0.3235,  0.3111, -0.4155,\n",
      "        -0.3803, -0.5710, -0.4365, -0.4039, -0.4367, -0.3881,  0.3404,  0.2228,\n",
      "         0.0388, -0.3845, -0.5016, -0.3018, -0.3927,  0.3477, -0.4365, -0.4336,\n",
      "         0.2944, -0.2660, -0.2943,  0.2315,  0.3128,  0.2630, -0.6035, -0.4072,\n",
      "        -0.1877,  0.1289, -0.4284, -0.0302, -0.3874,  0.3771, -0.2025, -0.2856,\n",
      "         0.3757, -0.2708,  0.3774,  0.3527,  0.3040,  0.3169, -0.0320,  0.3810,\n",
      "        -0.2149, -0.4027, -0.4062, -0.4527,  0.2377,  0.3638,  0.3194, -0.4183,\n",
      "         0.3532, -0.4073, -0.3752,  0.4227, -0.3647,  0.3111, -0.4131,  0.3724,\n",
      "         0.3450, -0.4310,  0.3579, -0.3957, -0.4081,  0.2979,  0.3680, -0.3950,\n",
      "        -0.4267, -0.3929, -0.3995, -0.3515, -0.4115, -0.2894, -0.0952, -0.3984,\n",
      "        -0.3351,  0.3309,  0.3893, -0.3171, -0.3869,  0.3853, -0.3410,  0.2555,\n",
      "        -0.4161, -0.4080, -0.4238, -0.3106,  0.3327, -0.1182,  0.1522, -0.4133,\n",
      "        -0.4188, -0.0103, -0.4343,  0.1414,  0.2134,  0.3050, -0.3954, -0.3894,\n",
      "        -0.3724, -0.3879, -0.3254, -0.3741, -0.4094, -0.4218,  0.2528,  0.2836,\n",
      "         0.2713, -0.3264,  0.3348,  0.3082,  0.0037,  0.2880, -0.3945,  0.2596,\n",
      "        -0.1117, -0.0779, -0.3715,  0.3385, -0.4052, -0.0018, -0.4022,  0.3702,\n",
      "        -0.4241, -0.2209, -0.3426,  0.1865, -0.5408, -0.1398,  0.2934, -0.4184,\n",
      "         0.2897, -0.2547,  0.3984, -0.3798, -0.3111, -0.2306, -0.4051,  0.3698,\n",
      "        -0.3471, -0.4347, -0.1433,  0.3639, -0.4418, -0.3366, -0.3913, -0.2881,\n",
      "        -0.1425,  0.3366,  0.2960,  0.3575, -0.4013,  0.3455,  0.3300,  0.3675,\n",
      "         0.2820, -0.2571,  0.3274, -0.0814,  0.2705,  0.3694, -0.2681, -0.1623,\n",
      "         0.2646, -0.4308, -0.4176,  0.2658,  0.3618,  0.3799,  0.2357, -0.3617,\n",
      "         0.1491, -0.3936, -0.3886,  0.3307, -0.4184,  0.3290,  0.2415, -0.0366,\n",
      "        -0.2915,  0.3368,  0.3712, -0.3859, -0.1139, -0.4062, -0.3973, -0.3808,\n",
      "        -0.0613,  0.2268,  0.1015, -0.4045, -0.3651,  0.3263, -0.1710, -0.3186,\n",
      "        -0.4082, -0.5155, -0.2089, -0.4118,  0.2523,  0.3974,  0.2398, -0.4029,\n",
      "        -0.3671, -0.4152,  0.3978, -0.3351, -0.2503, -0.3977, -0.2906, -0.3471,\n",
      "        -0.3639,  0.5660,  0.3811, -0.4425, -0.6028,  0.0873,  0.3924,  0.3375,\n",
      "         0.3974, -0.4078, -0.4050, -0.3405,  0.3895, -0.3789, -0.4238,  0.3599,\n",
      "        -0.2583,  0.3065, -0.4644,  0.2821, -0.2259, -0.6385, -0.3507, -0.1260,\n",
      "        -0.4099,  0.5399, -0.4145,  0.1030, -0.3863,  0.1632, -0.1430, -0.3938,\n",
      "        -0.3792, -0.4040, -0.3339, -0.2606, -0.3857,  0.3632,  0.3853,  0.2217,\n",
      "        -0.3993,  0.3282,  0.3092, -0.1501,  0.1426, -0.0363,  0.3528,  0.3022,\n",
      "        -0.3687, -0.4306, -0.3999,  0.1877,  0.3272, -0.4389, -0.4432, -0.2229,\n",
      "         0.3026, -0.3720,  0.3870,  0.3096, -0.1449,  0.3668,  0.2698,  0.2367,\n",
      "        -0.1680, -0.2234, -0.3841, -0.4051,  0.3680, -0.4036, -0.4104,  0.3532,\n",
      "        -0.0817, -0.4018, -0.3122, -0.4048,  0.3020, -0.4260,  0.2231,  0.3181,\n",
      "        -0.3806, -0.4249, -0.3920, -0.3308, -0.0182,  0.3684, -0.4260, -0.4126,\n",
      "        -0.4318, -0.1753, -0.3003, -0.3144, -0.4112,  0.0200, -0.4348,  0.2935,\n",
      "        -0.4051,  0.0094, -0.4119,  0.3052, -0.3680, -0.3834,  0.3499, -0.3736,\n",
      "        -0.3336, -0.3959, -0.3255, -0.2511, -0.0847,  0.3959, -0.4085, -0.4650,\n",
      "         0.3691, -0.2161, -0.4728,  0.2704, -0.4311,  0.3246, -0.2950, -0.3962,\n",
      "        -0.4022,  0.3767,  0.3519, -0.3228, -0.3830,  0.3222,  0.3184, -0.3125,\n",
      "         0.0870, -0.3888, -0.3203,  0.3248,  0.3330,  0.2489,  0.3581, -0.4146,\n",
      "         0.4939, -0.1426, -0.0451,  0.2834, -0.2741, -0.0672, -0.4014, -0.3032,\n",
      "         0.3241,  0.3976, -0.1487,  0.3049,  0.0279, -0.4137,  0.2961,  0.2471],\n",
      "       device='cuda:3')), ('my_layers.2.2.weight', tensor([0.0088], device='cuda:3')), ('my_layers.3.weight', tensor([[-4.9999e-03, -4.6303e-03, -6.0199e-03,  5.1978e-03,  1.9002e-03,\n",
      "          1.9245e-03, -2.5436e-03,  7.6587e-03,  1.7497e-03,  5.5928e-03,\n",
      "         -4.9270e-03, -4.3173e-03,  3.5294e-03, -5.6490e-03,  1.9283e-03,\n",
      "          2.0062e-03,  3.0592e-03, -4.6322e-03, -6.1120e-03, -5.8182e-03,\n",
      "          3.0445e-03, -7.4055e-03,  2.1163e-03,  2.5370e-03,  1.6692e-03,\n",
      "          1.7424e-03,  2.0654e-03, -3.7630e-03,  1.6155e-03,  2.0576e-03,\n",
      "         -5.3596e-03,  2.1042e-03, -5.1869e-03, -7.2123e-03,  7.4079e-03,\n",
      "         -3.8584e-03,  2.1537e-03,  5.7923e-03,  3.3111e-03, -5.4839e-03,\n",
      "         -5.8317e-03,  2.8798e-03, -4.1066e-03,  6.5005e-03, -4.4798e-03,\n",
      "         -4.2948e-03,  2.1324e-03,  5.8407e-03,  6.8637e-03,  9.9065e-04,\n",
      "         -4.5937e-03,  1.0291e-03,  2.0436e-03, -1.6309e-03,  1.6730e-03,\n",
      "         -2.4694e-03, -5.5671e-03, -5.8157e-03, -4.1749e-03, -4.3161e-03,\n",
      "          2.7822e-03,  1.7051e-03,  1.0107e-02,  2.3740e-03, -8.4175e-03,\n",
      "         -4.6801e-03,  8.7704e-03, -4.8627e-03,  2.5619e-03,  2.8164e-03,\n",
      "          2.8936e-03, -4.1837e-03, -5.3116e-03,  1.1918e-03, -4.8898e-03,\n",
      "         -2.2178e-03,  1.6375e-03, -5.4885e-03,  2.3370e-03,  8.3910e-03,\n",
      "         -5.4616e-03,  2.4661e-03, -7.2657e-03, -4.7104e-03, -2.6206e-03,\n",
      "          1.7436e-03, -3.8248e-03,  8.1044e-03,  2.4002e-03, -5.7700e-03,\n",
      "          1.8401e-03,  6.5827e-03,  1.4012e-03, -5.0311e-03, -4.8873e-03,\n",
      "          2.1623e-03,  8.0133e-03, -4.3680e-03,  1.0681e-03,  1.4736e-03,\n",
      "         -4.3854e-03, -5.2295e-03, -4.1302e-03,  2.3694e-03,  2.5095e-03,\n",
      "          2.0496e-03, -5.3590e-03, -2.8632e-03, -1.9410e-03, -4.0066e-03,\n",
      "          1.9595e-03, -6.4957e-03, -3.3381e-03, -7.0202e-03,  4.9404e-03,\n",
      "         -1.8062e-03,  2.3708e-03,  1.5856e-03, -4.9132e-03,  1.1468e-03,\n",
      "          1.4489e-03, -3.6806e-03, -8.6823e-03, -5.5133e-03, -4.2584e-03,\n",
      "         -5.6277e-03,  2.1914e-03,  1.1213e-03,  2.1082e-03, -5.8753e-03,\n",
      "         -5.7394e-03,  2.2823e-03, -5.2394e-03, -9.8385e-03,  2.1275e-03,\n",
      "          3.7150e-03, -2.0531e-03, -4.0686e-03, -3.8088e-03, -5.7543e-03,\n",
      "         -5.0947e-03,  1.3891e-03,  1.1814e-03, -5.5971e-03,  1.0536e-03,\n",
      "         -2.9423e-03,  1.6197e-03,  2.1682e-03,  1.9042e-03,  5.6394e-03,\n",
      "         -4.1332e-03, -3.6223e-03, -3.1219e-03, -5.8640e-03, -6.0825e-03,\n",
      "         -4.1443e-03, -4.3847e-03,  1.9200e-03, -4.9501e-03,  2.6001e-03,\n",
      "         -6.5428e-03,  7.7824e-03, -3.0978e-03,  6.2228e-03,  6.9318e-04,\n",
      "         -7.9756e-04,  2.2378e-03, -3.9890e-03, -8.4496e-04,  1.4083e-03,\n",
      "          2.3921e-03,  6.2288e-03,  1.6405e-03,  1.7569e-03,  2.2265e-03,\n",
      "          6.7409e-03, -4.0171e-03,  2.5071e-03,  1.8412e-03,  1.9738e-03,\n",
      "          7.3470e-03, -3.1995e-03, -5.8777e-03,  1.9781e-03,  9.8118e-03,\n",
      "         -3.5118e-03,  1.6581e-03, -4.5561e-03,  5.9696e-03, -3.5640e-03,\n",
      "         -8.6442e-03, -5.4696e-03,  1.6972e-03, -5.5797e-03,  8.3034e-03,\n",
      "         -4.7393e-03,  3.9971e-03, -3.7382e-03, -5.4553e-03, -4.4359e-03,\n",
      "         -5.3468e-03,  2.1077e-03, -5.6624e-03, -4.8763e-03, -4.5687e-03,\n",
      "          8.8695e-03,  7.3003e-03,  2.3901e-03,  7.0789e-03,  3.5781e-03,\n",
      "          3.6058e-03, -3.8496e-03, -3.7412e-03, -5.4411e-03, -5.1873e-03,\n",
      "         -5.2657e-03, -4.4238e-03, -4.2289e-03,  2.5926e-03, -5.8188e-03,\n",
      "         -4.6080e-03, -6.9674e-03, -7.0846e-03, -6.9930e-03,  4.9843e-03,\n",
      "          4.4882e-03, -4.9660e-03, -3.4955e-03, -4.4662e-03,  7.3017e-03,\n",
      "          7.0668e-03, -6.3898e-03,  1.7729e-03,  2.8835e-03,  8.4863e-04,\n",
      "         -3.3942e-03,  2.1387e-03, -2.1850e-03, -5.1010e-03, -3.5946e-03,\n",
      "          1.5630e-03, -5.6374e-03, -6.3026e-03, -8.3917e-03,  2.2886e-03,\n",
      "         -3.9145e-03, -6.1470e-03, -6.2661e-03, -5.5390e-03,  2.4000e-03,\n",
      "          2.8020e-03, -3.8924e-03,  2.1449e-03,  1.4666e-03,  1.7749e-03,\n",
      "         -5.4927e-03, -3.6100e-03,  1.3436e-03,  8.7655e-04,  2.1149e-03,\n",
      "          1.2058e-02, -5.5205e-03, -4.4913e-03, -5.1840e-03, -2.9505e-03,\n",
      "          2.2865e-03, -4.0256e-03,  1.6714e-03, -4.1800e-03,  1.7672e-03,\n",
      "         -4.5217e-03, -4.2044e-03, -4.7572e-03,  1.3772e-03, -3.9864e-03,\n",
      "          1.5531e-03, -5.2103e-03,  1.5121e-03,  1.0732e-03,  8.8574e-03,\n",
      "         -6.2396e-03, -5.1681e-03, -6.2146e-03, -5.9086e-03, -4.9584e-03,\n",
      "         -4.3446e-03,  7.8685e-03,  1.4525e-03,  4.9106e-03, -5.9813e-03,\n",
      "         -3.1660e-03, -3.3214e-03,  2.1296e-03,  9.0330e-04,  1.7545e-03,\n",
      "         -3.9613e-03,  1.0699e-03, -4.4659e-03, -3.8063e-03, -3.8978e-03,\n",
      "         -2.4566e-03,  2.6461e-03,  1.0830e-02, -4.7340e-03, -3.4864e-03,\n",
      "         -5.3661e-03,  3.5567e-03,  7.2348e-03, -4.3985e-03, -5.2708e-03,\n",
      "          6.6895e-04, -6.4126e-03,  6.5996e-03, -5.4703e-03,  6.2828e-03,\n",
      "         -4.2503e-03, -4.9609e-03, -4.3882e-03, -3.0291e-03,  1.9413e-03,\n",
      "          1.2793e-03, -6.4506e-03, -6.3876e-03, -2.8486e-03,  1.6631e-03,\n",
      "         -2.3288e-03, -4.3441e-03,  4.7191e-03,  1.5494e-03, -4.9323e-03,\n",
      "         -4.9161e-03,  1.6958e-03,  9.5584e-04,  4.2044e-03, -5.7836e-03,\n",
      "         -5.0043e-03, -5.9231e-03, -4.8380e-03,  2.7778e-03,  9.0779e-04,\n",
      "         -4.2439e-03, -4.2411e-03, -5.8540e-03, -4.2316e-03,  1.8530e-03,\n",
      "          1.3504e-03,  1.6533e-03, -3.3540e-03,  1.4014e-03,  6.8407e-03,\n",
      "         -9.4663e-03,  1.3205e-03,  8.6732e-03,  1.6690e-03,  1.7895e-03,\n",
      "          2.2580e-03, -4.1602e-03,  2.0192e-03, -5.0307e-03, -4.6947e-03,\n",
      "         -4.8646e-03,  2.3651e-03, -5.3398e-03,  7.3080e-03,  1.3737e-03,\n",
      "          6.7597e-03,  1.7425e-03,  6.5907e-03,  1.0175e-02, -6.2689e-03,\n",
      "         -4.6885e-03,  1.7052e-03, -2.5217e-03,  1.5085e-03,  1.4979e-03,\n",
      "         -4.7120e-03, -5.0989e-03,  3.7150e-03, -4.4314e-03, -5.3317e-03,\n",
      "         -3.5900e-03, -4.0027e-03,  2.2266e-03,  1.2147e-03,  2.3058e-03,\n",
      "         -3.5661e-03,  5.3277e-03, -6.4498e-03, -3.9419e-03,  1.8678e-03,\n",
      "         -3.0315e-03, -4.3760e-03,  2.4539e-03,  8.7278e-03,  7.9736e-03,\n",
      "          2.3073e-03,  1.3546e-03,  1.7302e-03, -5.7589e-03, -6.3940e-03,\n",
      "          7.2039e-03,  3.7943e-03, -3.7148e-03,  1.0821e-02, -4.0610e-03,\n",
      "          3.0647e-03, -7.2619e-03, -5.0610e-03,  2.1098e-03, -5.0674e-03,\n",
      "          1.9669e-03,  2.2417e-03,  1.5680e-03,  2.0706e-03,  2.4099e-03,\n",
      "          2.0889e-03, -2.8707e-03, -4.9680e-03, -4.2706e-03,  7.6641e-03,\n",
      "          1.7534e-03,  1.6558e-03,  1.3426e-03, -6.3768e-03,  1.7411e-03,\n",
      "         -4.4517e-03, -5.0507e-03, -2.7040e-03, -5.0515e-03,  1.1790e-03,\n",
      "         -5.0938e-03,  1.7911e-03,  2.0158e-03,  5.4442e-03,  1.8275e-03,\n",
      "         -3.4401e-03, -4.9264e-03,  1.0250e-03,  2.6056e-03, -3.9898e-03,\n",
      "         -5.2751e-03, -2.7297e-03,  5.4431e-03, -5.1995e-03, -6.4624e-03,\n",
      "         -4.9501e-03, -6.9501e-03, -3.1611e-03, -4.9132e-03,  1.9592e-03,\n",
      "          1.9821e-03, -5.5970e-03, -6.1088e-03,  1.1892e-03, -3.3982e-03,\n",
      "          1.2769e-03, -4.4030e-03, -3.4762e-03, -3.7873e-03, -5.9572e-03,\n",
      "          3.3761e-03,  2.2745e-03,  1.9958e-03, -2.8416e-03, -4.1461e-03,\n",
      "          2.6733e-03, -1.7136e-03,  2.2613e-03,  1.7321e-03,  1.6655e-03,\n",
      "         -3.6407e-03, -6.5945e-03, -4.9167e-03, -4.0837e-03, -4.7787e-03,\n",
      "         -5.0837e-03, -4.7101e-03, -5.4781e-03,  9.2912e-04,  1.7483e-03,\n",
      "          1.8516e-03,  7.7569e-03,  9.6743e-04,  1.4319e-03,  2.2814e-03,\n",
      "          1.2771e-03, -5.7405e-03,  6.9466e-04, -2.4240e-03, -2.4855e-03,\n",
      "         -2.1265e-03,  1.2709e-03, -4.0765e-03,  1.6013e-03, -5.5103e-03,\n",
      "          2.4249e-03, -3.8634e-03, -4.0585e-03, -6.3345e-03,  4.5571e-04,\n",
      "         -4.7302e-03, -5.6297e-03,  1.6492e-03,  5.6758e-03,  1.7732e-03,\n",
      "         -5.5578e-03,  2.0267e-03, -4.4425e-03, -3.6373e-03, -6.2550e-03,\n",
      "         -4.8579e-03,  1.9137e-03, -3.6042e-03, -2.8091e-03, -8.2808e-04,\n",
      "          2.3329e-03, -4.0891e-03, -4.0278e-03, -2.1947e-03, -6.3720e-03,\n",
      "         -6.9041e-03,  1.7862e-03,  1.5430e-03,  2.3220e-03, -2.3342e-03,\n",
      "          1.3823e-03,  1.8490e-03,  1.4541e-03,  1.4170e-03, -5.7406e-03,\n",
      "          1.6184e-03, -1.8279e-03,  1.7903e-03,  2.3161e-03, -4.5638e-03,\n",
      "          7.4922e-03,  1.4240e-03, -4.6763e-03, -5.2484e-03,  1.4063e-03,\n",
      "          1.9719e-03,  2.2037e-03,  6.3123e-05, -3.4469e-03,  5.0131e-03,\n",
      "         -4.1322e-03, -5.3902e-03,  1.7409e-03, -4.2972e-03,  1.8871e-03,\n",
      "          1.7585e-04, -5.7343e-03, -5.7925e-03,  7.8669e-04,  2.4542e-03,\n",
      "         -4.0684e-03, -4.6285e-03, -3.4163e-03, -3.8935e-03, -4.9336e-03,\n",
      "         -2.9577e-03,  3.7502e-03,  7.4975e-03, -5.0702e-03, -5.3500e-03,\n",
      "          3.4992e-03, -6.1369e-03, -5.1279e-03, -4.1474e-03,  5.0718e-03,\n",
      "         -3.4869e-03, -3.8749e-03,  8.8194e-04,  3.0323e-03,  3.2039e-03,\n",
      "          9.8268e-03, -5.1957e-03, -7.1167e-03,  3.6769e-03, -2.8100e-03,\n",
      "          9.5421e-03, -3.9098e-03,  1.6336e-03, -4.3475e-03, -4.2297e-03,\n",
      "         -2.3125e-03,  1.8256e-03, -4.7918e-03, -5.6968e-03,  3.6563e-03,\n",
      "          2.2552e-03,  2.9010e-03, -2.6687e-03, -4.7093e-03, -4.8132e-03,\n",
      "         -5.2427e-03,  2.1026e-03, -5.2228e-03, -4.4308e-03,  2.7029e-03,\n",
      "          5.1282e-03,  2.5479e-03, -5.0885e-03,  1.9609e-03,  8.9473e-03,\n",
      "          2.0845e-03,  5.8417e-03, -5.6477e-03, -4.5815e-03, -2.9365e-03,\n",
      "         -4.5335e-03, -5.8425e-03, -6.2581e-03,  4.1595e-03,  5.5368e-03,\n",
      "         -4.3369e-03, -5.5322e-03,  6.5342e-03, -5.5408e-03,  6.6330e-03,\n",
      "         -4.3600e-03,  1.3334e-03,  2.2980e-03,  1.3469e-03, -3.7743e-03,\n",
      "          1.2612e-03,  1.7508e-03, -3.2425e-03,  9.3029e-03,  8.6267e-04,\n",
      "          2.1101e-03,  1.8760e-03, -4.4318e-03, -4.4380e-03, -5.3505e-03,\n",
      "          5.5738e-04,  2.0767e-03, -4.6793e-03, -3.7264e-03,  9.5126e-03,\n",
      "          2.0335e-03, -2.7186e-03,  2.7317e-03,  1.6907e-03, -6.6732e-03,\n",
      "          2.4289e-03,  1.5722e-03,  1.5052e-03,  3.6566e-03, -6.7835e-03,\n",
      "         -5.6153e-03, -5.6453e-03,  2.5937e-03, -4.6284e-03, -3.8605e-03,\n",
      "         -2.4422e-03, -4.0239e-03, -6.2966e-03, -1.5532e-03, -6.5101e-03,\n",
      "          1.9383e-03, -4.1845e-03,  9.5511e-04,  2.1302e-03, -5.0656e-03,\n",
      "         -5.7215e-03, -6.3746e-03, -4.8010e-03, -3.2483e-03,  2.4554e-03,\n",
      "         -3.8527e-03, -3.4103e-03, -4.5737e-03, -4.9543e-03, -4.7834e-03,\n",
      "         -5.3274e-03, -3.7919e-03, -6.1229e-03, -4.4445e-03,  2.3537e-03,\n",
      "         -4.5295e-03,  1.7804e-03, -3.0080e-03,  1.3198e-03, -6.2982e-03,\n",
      "         -3.8283e-03,  2.1346e-03, -5.6127e-03, -4.6281e-03, -5.9424e-03,\n",
      "         -6.4376e-03, -5.6297e-03,  7.7070e-03,  1.3354e-03, -3.4081e-03,\n",
      "         -5.9156e-03,  1.5407e-03, -3.6517e-03, -4.6194e-03,  2.9294e-03,\n",
      "         -5.9127e-03,  1.2953e-03, -4.9917e-03, -3.7406e-03, -2.5287e-03,\n",
      "          1.9347e-03,  2.1600e-03, -6.2303e-03, -4.4833e-03,  3.2132e-03,\n",
      "          2.3490e-03, -4.9217e-03,  1.9275e-03, -5.4966e-03, -5.3508e-03,\n",
      "          2.2397e-03,  1.6934e-03,  6.6258e-04,  3.4379e-03, -3.7883e-03,\n",
      "         -1.9060e-03, -5.5199e-03,  2.7962e-03,  1.9070e-03,  7.0736e-03,\n",
      "          8.5190e-03, -3.6867e-03, -5.6253e-03,  1.5527e-03,  1.8408e-03,\n",
      "         -8.5848e-03,  9.8822e-04,  1.1714e-02, -5.6069e-03,  1.0657e-03,\n",
      "          1.5701e-03]], device='cuda:3')), ('my_layers.3.bias', tensor([0.3093], device='cuda:3'))])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('../models/molecule_polyBERT.pth')\n",
    "state_dict.pop(\"my_layers.3.weight\")\n",
    "state_dict.pop(\"my_layers.3.bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = net()\n",
    "    layers.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "    for name, param in layers.named_parameters():\n",
    "        if '0.0' in name or '0.2' in name or '1.0' in name or '1.2' in name or '2.0' in name or '2.2' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    in_features = 736\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 352, 2144, step=64, log = False)\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.1, 0.5, log = True)\n",
    "\n",
    "        new_step =[\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.Dropout(p),\n",
    "            nn.PReLU()\n",
    "            ]\n",
    "        layers.append(nn.Sequential(*new_step))\n",
    "       \n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Generate the model.\n",
    "    \n",
    "    model = define_model(trial)    \n",
    "    model.to(DEVICE)\n",
    "  \n",
    "    # Generate the optimizers.\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log = True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    EPOCHS  = trial.suggest_int(\"EPOCHS\", 50, 700)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx,(data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.view(-1).to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.mse_loss(output.view(-1), target)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss = running_loss / batch_idx\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                # Limiting validation data.\n",
    "\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                pred = model(data)\n",
    "                target_scaled = scalar.inverse_transform(target.cpu().numpy())\n",
    "                pred_scaled = scalar.inverse_transform(pred.cpu().detach().numpy())\n",
    "\n",
    "                test_loss = mean_squared_error(target_scaled, pred_scaled)\n",
    "                val_loss  += test_loss\n",
    "\n",
    "        avg_val_loss = val_loss/ batch_idx \n",
    "\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials = 25000, n_jobs= 10)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    # Save results to csv file\n",
    "    df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Exclude columns\n",
    "    df = df.loc[df['state'] == 'COMPLETE']        # Keep only results that did not prune\n",
    "    df = df.drop('state', axis=1)                 # Exclude state column\n",
    "    df = df.sort_values('value')                  # Sort based on accuracy\n",
    "    df.to_csv('op_add.csv', index=False)  # Save to csv file\n",
    "\n",
    "    # Display results in a dataframe\n",
    "    print(\"\\nOverall Results (ordered by accuracy):\\n {}\".format(df))\n",
    "\n",
    "    # Find the most important hyperparameters\n",
    "    most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
    "\n",
    "    # Display the most important hyperparameters\n",
    "    print('\\nMost important hyperparameters:')\n",
    "    for key, value in most_important_parameters.items():\n",
    "        print('  {}:{}{:.2f}%'.format(key, (15-len(key))*' ', value*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
