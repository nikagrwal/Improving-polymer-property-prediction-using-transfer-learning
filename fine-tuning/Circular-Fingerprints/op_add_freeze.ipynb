{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from psmiles import PolymerSmiles as PS\n",
    "from sklearn.metrics  import  mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from optuna.trial import TrialState \n",
    "\n",
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fingerprint(smiles):\n",
    "    fingerprint = np.array(PS(smiles).fingerprint_circular)\n",
    "    return fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/polymers.csv\")\n",
    "desired_columns = [\"smiles\",\"value\"]\n",
    "df = df[df[\"property\"] == \"Egc\"]\n",
    "df = df[desired_columns]\n",
    "df.rename(columns={'value':'Egc'}, inplace= True)\n",
    "df[\"fingerprint\"] = df[\"smiles\"].apply(generate_fingerprint)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = MinMaxScaler()\n",
    "data = df[\"fingerprint\"]\n",
    "target = df[\"Egc\"]\n",
    "\n",
    "#data = data.values.reshape(-1, 1)  # Reshape data\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=123)\n",
    "\n",
    "# Scaling target variable\n",
    "target_train = scalar.fit_transform(target_train.values.reshape(-1, 1))\n",
    "target_test = scalar.transform(target_test.values.reshape(-1, 1))\n",
    "\n",
    "# Creating tensors from data\n",
    "\n",
    "#Training Data\n",
    "data_train_tensor = torch.tensor(data_train.reset_index(drop = True), dtype=torch.float32)\n",
    "target_train_tensor = torch.tensor(target_train, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(data_train_tensor, target_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size= 32, shuffle= True)\n",
    "\n",
    "#Testing Data\n",
    "\n",
    "data_test_tensor = torch.tensor(data_test.reset_index(drop= True), dtype=torch.float32)\n",
    "target_test_tensor = torch.tensor(target_test, dtype=torch.float32)\n",
    "\n",
    "test_dataset = TensorDataset(data_test_tensor, target_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net():\n",
    "    layers =  nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(2048, 1888),\n",
    "                nn.Dropout(0.296708814),\n",
    "                nn.PReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1888, 416),\n",
    "                nn.Dropout(0.103316943),\n",
    "                nn.PReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(416, 1632),\n",
    "                nn.Dropout(0.178598433),\n",
    "                nn.PReLU()\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../models/molecule_circular.pth')\n",
    "state_dict.pop(\"my_layers.3.weight\")\n",
    "state_dict.pop(\"my_layers.3.bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = net()\n",
    "    layers.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    freeze_1 = trial.suggest_categorical(\"freeze\", [\"freeze\",\"no freeze\"])\n",
    "    if freeze_1 == \"freeze\":\n",
    "        for name, param in layers.named_parameters():\n",
    "            if '0.0' in name or '0.2' in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    freeze_2 = trial.suggest_categorical(\"freeze\", [\"freeze\",\"no freeze\"])\n",
    "    if freeze_2 == \"freeze\":\n",
    "        for name, param in layers.named_parameters():\n",
    "            if '0.0' in name or '0.2' in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    freeze_3 = trial.suggest_categorical(\"freeze\", [\"freeze\",\"no freeze\"])\n",
    "    if freeze_3 == \"freeze\":\n",
    "        for name, param in layers.named_parameters():\n",
    "            if '0.0' in name or '0.2' in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        \n",
    "\n",
    "    in_features = 1632\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 352, 2144, step=64, log = False)\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.1, 0.5, log = True)\n",
    "\n",
    "        new_step =[\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.Dropout(p),\n",
    "            nn.PReLU()\n",
    "            ]\n",
    "        layers.append(nn.Sequential(*new_step))\n",
    "       \n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Generate the model.\n",
    "    \n",
    "    model = define_model(trial)    \n",
    "    model.to(DEVICE)\n",
    "  \n",
    "    # Generate the optimizers.\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log = True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    EPOCHS  = trial.suggest_int(\"EPOCHS\", 50, 700)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx,(data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.view(-1).to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.mse_loss(output.view(-1), target)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss = running_loss / batch_idx\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                # Limiting validation data.\n",
    "\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                pred = model(data)\n",
    "                target_scaled = scalar.inverse_transform(target.cpu().numpy())\n",
    "                pred_scaled = scalar.inverse_transform(pred.cpu().detach().numpy())\n",
    "\n",
    "                test_loss = mean_squared_error(target_scaled, pred_scaled)\n",
    "                val_loss  += test_loss\n",
    "\n",
    "        avg_val_loss = val_loss/ batch_idx \n",
    "\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials = 25000, n_jobs= 10)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    # Save results to csv file\n",
    "    df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Exclude columns\n",
    "    df = df.loc[df['state'] == 'COMPLETE']        # Keep only results that did not prune\n",
    "    df = df.drop('state', axis=1)                 # Exclude state column\n",
    "    df = df.sort_values('value')                  # Sort based on accuracy\n",
    "    df.to_csv('op_add_freeze.csv', index=False)  # Save to csv file\n",
    "\n",
    "    # Display results in a dataframe\n",
    "    print(\"\\nOverall Results (ordered by accuracy):\\n {}\".format(df))\n",
    "\n",
    "    # Find the most important hyperparameters\n",
    "    most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
    "\n",
    "    # Display the most important hyperparameters\n",
    "    print('\\nMost important hyperparameters:')\n",
    "    for key, value in most_important_parameters.items():\n",
    "        print('  {}:{}{:.2f}%'.format(key, (15-len(key))*' ', value*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
